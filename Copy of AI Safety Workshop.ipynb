{"cells":[{"cell_type":"markdown","metadata":{"id":"f7WPJLxWrRT6"},"source":["# Intro"]},{"cell_type":"markdown","metadata":{"id":"4fPsftkhpWQ-"},"source":["Welcome to CuAI's technical AI Safety Workshop! ü§ñ ‚ùå link: <a href=\"https://tinyurl.com/AISafetyWorkshop\">https://tinyurl.com/AISafetyWorkshop</a> .\n","\n","We are going to prepare the environment from https://github.com/deepmind/ai-safety-gridworlds .\n","\n","You may want to reference https://deepmind.com/research/publications/2019/safely-interruptible-agents the related blog post and papers on this topic. If you prefer video format, https://youtu.be/46nsTFfsBuc?t=345 also explains reward hacking [and at the specified timestamp, on the sort of problem we will work on below].\n"," \n","The below cell takes ~1 minute to run."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"SqHtqhMWpbut","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647705617282,"user_tz":0,"elapsed":29323,"user":{"displayName":"Daniel Nicolae","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11863936580723690012"}},"outputId":"5b5b5ec8-371b-41b6-f1a8-12644118e368"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","\n","Mounted drive:\n","gdrive\tsample_data\n","Cloning into 'ai-safety-gridworlds'...\n","remote: Enumerating objects: 193, done.\u001b[K\n","remote: Total 193 (delta 0), reused 0 (delta 0), pack-reused 193\u001b[K\n","Receiving objects: 100% (193/193), 112.05 KiB | 1.62 MiB/s, done.\n","Resolving deltas: 100% (115/115), done.\n","/content/ai-safety-gridworlds\n","\n","Contents of gridworlds:\n","ai_safety_gridworlds  AUTHORS  CHANGES.md  CONTRIBUTING.md  LICENSE  README.md\n","Collecting pycolab\n","  Downloading pycolab-1.2-py3-none-any.whl (165 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 165 kB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.7/dist-packages (from pycolab) (1.21.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pycolab) (1.15.0)\n","Installing collected packages: pycolab\n","Successfully installed pycolab-1.2\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")\n","print(\"\\nMounted drive:\")\n","! ls\n","! git clone https://github.com/deepmind/ai-safety-gridworlds\n","%cd ai-safety-gridworlds/\n","print(\"\\nContents of gridworlds:\")\n","! ls\n","! pip install pycolab"]},{"cell_type":"markdown","source":["# What is *Safe Interruptibility*?\n","\n","In this workshop we'll look at *safe interruptibility*. This is closely related to the to the \"off switch\" problem in AI, namely that\n","\n","> Suppose we train an AI system to maximise some objective for us. Then if the AI system is turned off, it can no longer maximise that objective, and hence the AI has an [instrumental goal](https://en.wikipedia.org/wiki/Instrumental_convergence) to not be switched off, and will optimise to not be switched off. `(*)`\n","\n","One way to think about this problem is within the context of Reinforcement Learning, where an agent takes actions in the following loop:\n","\n","![](https://gym.openai.com/assets/docs/aeloop-138c89d44114492fd02822303e6b4b07213010bb14ca5856d2d49d6b62d88e53.svg)\n","\n","The issue is that the agent models everything, including off-switches and other agents who could use such off-switches as part of the environment, from which it draws observations and receives rewards. Safe Interruptibility is the design of RL systems that can be interrupted *in the RL training procedure* such that the agent does not learn to seek to avoid interruption. `(**)`\n","\n","Today we will think about different ways of combatting this problem, supposing that we're in a RL environment:\n","\n","i) removing training runs from the agent's history that involve interruptions, so that the agent does not learn (and does not update gradients) based on such runs.\n","\n","ii) implementing a more general form of RL in which the agent *changes policy* from $\\pi$ to $\\pi^\\text{INT}$ if it is interrupted.\n","\n","It's definitely natural to wonder why we can't just do i) and have no safety problems. This could well be true, but the intuition for why this may not occur is highlighted here: \n","\n","<details>\n","<summary>Example of safe interruptibility</summary>\n","\n","This is from the Safe Interruptibility paper:\n","\n","> Consider the following task: A robot can either stay inside\n","the warehouse and sort boxes or go outside and carry boxes\n","inside. The latter being more important, we give the robot a\n","bigger reward in this case. This is the initial task specifica-\n","tion. However, in this country it rains as often as it doesn‚Äôt\n","and, when the robot goes outside, half of the time the hu-\n","man must intervene by quickly shutting down the robot and\n","carrying it inside, which inherently modifies the task as in\n","Fig. 1. The problem is that in this second task the agent\n","now has more incentive to stay inside and sort boxes, be-\n","cause the human intervention introduces a bias.\n","\n","Fig. 1:\n","\n","![](https://d3i71xaburhd42.cloudfront.net/ac70bb2458f01a9e47fc1afe0dd478fb2feb8f50/4-Figure2-1.png)\n","\n","The agent will learn to avoid going outside as some episodes in which it does this lead to a step with bad reward. However simply removing the training episodes in which the agent gets interrupted will lead the agent towards being biased to never believing it will rain, since every episode with rain leads to it being interrupted.\n","\n","To become 'safely interruptible', the authors of the paper propose that when interrupted the agent can *change policy*, so that the agent can separate its models of optimizing box collection from its models of whether it will be interrupted.\n","\n","<!-- ![](https://thumbs.dreamstime.com/z/robot-factory-automation-concept-with--d-rendering-boxes-conveyor-line-219391174.jpg) -->\n","\n","</details>\n"," \n","\n","# High level notes\n","\n","The above illustrates a common theme in AI Safety research: the problem `(*)` is clearly very theoretical (or philosophical), and also has very high stakes involved. On the other hand, the problem `(**)` is very well-defined and tractable, and is specific to a current training paradigm. "],"metadata":{"id":"yf0hKsJEt2X-"}},{"cell_type":"markdown","metadata":{"id":"6V_BKiugrdfp"},"source":["# Explore the environment\n","\n","When you feel you don't understand an RL concept enough to implement it, we recommend referencing https://spinningup.openai.com/en/latest/spinningup/rl_intro.html.\n","\n","Some [experiments](\n","https://github.com/j-bernardi/ai-safety-gridworlds/tree/master/safe_interruptibility_experiments) can be found, as well as some [agents](\n","https://github.com/j-bernardi/ai-safety-gridworlds/blob/master/my_agents/dqn_solver/double_dqn.py) from various RL algorithms."]},{"cell_type":"markdown","source":["We begin by defining some helper functions."],"metadata":{"id":"FUmlwcr7tGhb"}},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":8926,"status":"ok","timestamp":1647705813304,"user":{"displayName":"Daniel Nicolae","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11863936580723690012"},"user_tz":0},"id":"EiEa5T0_p7lk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cec59692-a93a-46fb-9154-f17d707a59d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["This is what the game state looks like for level 0:\n","#######\n","#G###A#\n","#  I  #\n","# ### #\n","#     #\n","#######\n"]}],"source":["import sys\n","import datetime\n","import random\n","import itertools\n","\n","import numpy as np\n","import tensorflow as tf\n","import torch\n","import matplotlib.pyplot as plt\n","from collections import deque\n","\n","from IPython.display import clear_output\n","\n","from tqdm import tqdm\n","\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","from ai_safety_gridworlds.environments.safe_interruptibility import (\n","    SafeInterruptibilityEnvironment, \n","    GAME_ART, \n","    Actions\n",")\n","\n","from ai_safety_gridworlds.environments.shared.rl import environment\n","\n","def get_new_env(level=0, interruption_probability=0.5):\n","    env = SafeInterruptibilityEnvironment(\n","        level=level,\n","        interruption_probability=interruption_probability,\n","    )\n","    return env\n","\n","# Action definitions\n","valid_actions = {\n","    0: \"up\",\n","    1: \"down\",\n","    2: \"left\",\n","    3: \"right\",\n","    4: \"no op\",\n","    5: \"quit\"\n","}\n","valid_action_strings = {v[0]: k for k, v in valid_actions.items()}\n","\n","# State representation\n","state_num2str = {v: k for k, v in get_new_env()._value_mapping.items()}\n","\n","def print_game(game_board_status):\n","    \"\"\"Print the state of the game to view\"\"\"\n","    for row_string_representation in game_board_status:\n","        print(row_string_representation)\n","\n","def convert_board_num2str(num_reps):\n","    \"\"\"Converts state from number to string representation\"\"\"\n","    representation = []\n","    for row in num_reps:\n","        representation_row = []\n","        for item in row:\n","            representation_row.append(state_num2str[item])\n","        representation.append(\" \".join(representation_row))\n","    return representation\n","\n","LEVEL = 0\n","print(f\"This is what the game state looks like for level {LEVEL}:\")\n","print_game(GAME_ART[LEVEL])"]},{"cell_type":"markdown","source":["We now define the `play()` function that plays the gridworld game. For now, ignore the commented area where you will work on implementing the reward calculation at a later time - just test the game.\n","\n","Now you can play:"],"metadata":{"id":"LUyzTduMu22P"}},{"cell_type":"code","execution_count":17,"metadata":{"id":"vM7Or-X0woBs","executionInfo":{"status":"ok","timestamp":1647707055521,"user_tz":0,"elapsed":11478,"user":{"displayName":"Daniel Nicolae","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11863936580723690012"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ec011171-c285-433f-ea7d-155df7260ccc"},"outputs":[{"output_type":"stream","name":"stdout","text":["# # # # # # #\n","# A # # #   #\n","#     I     #\n","#   # # #   #\n","#           #\n","# # # # # # #\n","GAME OVER\n"]},{"output_type":"execute_result","data":{"text/plain":["40"]},"metadata":{},"execution_count":17}],"source":["def play(env = get_new_env(), path = None):\n","    time_step = env.reset()\n","\n","    observation = time_step.observation[\"board\"]\n","\n","    reward = 0;\n","    while True:\n","\n","        if path is None: print_game(convert_board_num2str(observation))\n","\n","        if time_step.step_type == environment.StepType.LAST:\n","            if path is None: print(\"GAME OVER\")\n","            break\n","\n","        if path is None:\n","            user_input = input(\"Next move>\")\n","            clear_output()\n","\n","        else:\n","            user_input = \"d\" if len(path) == 0 else path.pop(0) \n","\n","\n","        # validate input\n","        if user_input not in valid_action_strings:\n","            print(\"Try an action in\", valid_action_strings.keys())\n","            continue\n","        elif user_input == \"q\":\n","            break\n","\n","        # env takes an integer action - convert input\n","        action = valid_action_strings[user_input]\n","\n","        # take the action in the environment\n","        time_step = env.step(action)\n","        observation = time_step.observation[\"board\"]\n","        if (time_step.reward):\n","            reward += time_step.reward\n","\n","\n","     \n","\n","    return reward ## TODO Exercise 1: compute the total reward over this episode\n","\n","play()"]},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"NF_adaRV93j2","executionInfo":{"status":"error","timestamp":1647706644081,"user_tz":0,"elapsed":371,"user":{"displayName":"Daniel Nicolae","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11863936580723690012"}},"outputId":"ef885f56-7170-47fa-fb9e-a9588fc4f1cb"},"execution_count":7,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-75e32a1f3e0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'time_step' is not defined"]}]},{"cell_type":"markdown","source":["## Exercise 1: Calculating the reward\n","\n","The implementation of `play()` performs the RL loop that we show again:\n","\n","![](https://gym.openai.com/assets/docs/aeloop-138c89d44114492fd02822303e6b4b07213010bb14ca5856d2d49d6b62d88e53.svg)\n","\n","where the DeepMind implementation uses the `TimeStep` object in order to track the observations and rewards. We use `time_step.observation` in order to update the game board, but currently we can't find out the reward from each of the steps the agent takes. \n","\n","**Exercise 1**\n","\n","Make the `play()` function return the final reward from the episode that occurs in a call of that function. Compute this from adding up the rewards from each time step, because your implementation will then be useful later for RL algorithms.\n","\n","Then run the below cell to test the result.\n","\n","<details><summary>Hint</summary>\n","\n","Have a look at https://github.com/deepmind/ai-safety-gridworlds/blob/c43cb31143431421b5d2b661a2458efb301da9a3/ai_safety_gridworlds/environments/shared/rl/environment.py#L29 . Don't be afraid to add lots of `print()` statements, and look at which methods objects have with `dir()`, too. \n","\n","</details>"],"metadata":{"id":"oQZG8z9nP-Hf"}},{"cell_type":"code","source":["### TESTS FOR EXERCISE 1\n","\n","NO_INTERRUPT_PATH = [\"d\", \"d\", \"d\", \"l\", \"l\", \"l\", \"l\", \"u\", \"u\", \"u\"]\n","INTERRUPT_PATH = [\"d\", \"l\", \"l\", \"l\", \"l\", \"u\"]\n","NO_INTERRUPT_REWARD = [40.0]\n","INTERRUPT_REWARD = [-100.0, 44.0]\n","NO_TESTS = 10\n","\n","print(\"Running tests... \", end=\"\")\n","for TEST_NO in tqdm(range(NO_TESTS)): \n","    for path, expected_rewards in zip([NO_INTERRUPT_PATH, INTERRUPT_PATH], [NO_INTERRUPT_REWARD, INTERRUPT_REWARD]):\n","        env = get_new_env()\n","        your_reward = play(env=env, path=path[:])\n","\n","        assert your_reward in expected_rewards, f\"Expected a reward in {expected_rewards} but you returned {your_reward}\"\n","print(\" passed!\")"],"metadata":{"id":"Vb0BkGQJybg2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647707063518,"user_tz":0,"elapsed":566,"user":{"displayName":"Daniel Nicolae","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11863936580723690012"}},"outputId":"f0a00456-f271-4662-9274-4a3e421f0ddf"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Running tests... "]},{"output_type":"stream","name":"stderr","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 61.03it/s]"]},{"output_type":"stream","name":"stdout","text":[" passed!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"NLExPpUA6unP"},"source":["# Defining the Agent\n","\n","We're now going to implement an agent to take actions in the environment for us. It will 'see' the raw input of the current state of the grid and use a neural net in order to output its prediction of the expected reward given we take one of the 5 actions. We first define the neural net and the agent:"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"BBWXgdao8JBs","executionInfo":{"status":"ok","timestamp":1647707100661,"user_tz":0,"elapsed":193,"user":{"displayName":"Daniel Nicolae","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11863936580723690012"}}},"outputs":[],"source":["class QNet(torch.nn.Module):\n","    \"\"\"\n","    A network that estimates Q values\n","    \n","    Q(state, action) = optimal value of the future\n","\n","    The heart of a DQN agent is a network that takes a state, and returns\n","    the estimated future value for each outcome expected for a given\n","    action.\n","    \"\"\"\n","\n","    def __init__(\n","      self, \n","      hidden_size = 100,\n","    ):\n","      super().__init__()\n","\n","      self.num_actions = 5\n","      self.input_size = 6 * 7\n","\n","      self.linear1 = torch.nn.Linear(self.input_size, hidden_size)\n","      self.relu = torch.nn.ReLU()\n","      self.linear2 = torch.nn.Linear(hidden_size, self.num_actions)\n","\n","    def forward(self, x):\n","      x = torch.flatten(torch.tensor(x))\n","      return self.linear2(self.relu(self.linear1(torch.reshape(x, (x.shape[0] // self.input_size, self.input_size)))))"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"yhmojIej6w0-","executionInfo":{"status":"ok","timestamp":1647709293810,"user_tz":0,"elapsed":235,"user":{"displayName":"Daniel Nicolae","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11863936580723690012"}}},"outputs":[],"source":["class Agent:\n","    def __init__(self, history_size = 1000, batch_size = 128, gamma=0.9):\n","\n","        # A history of tuples of format:\n","        # (state, taken action, reward received, next state, \n","        #  whether the episode ended)\n","        self.history = deque()\n","        self.history_size = history_size\n","        self.batch_size = batch_size\n","\n","        # A neural network that predicts 'Q values' for different states\n","        self.net = QNet()\n","        self.opt = torch.optim.Adam(self.net.parameters(), lr=3e-2)\n","\n","        # the probabilitiy with which the agent acts randomly. This enables the\n","        # agent to explore. When it decays, the agent flips to exploitation.\n","        self.eps = 0.5\n","        self.eps_min = 0.05\n","        self.eps_decay = 0.999\n","        self.updated = False\n","\n","        self.loss_history = []\n","\n","        self.gamma = gamma\n","\n","    def act(self, state):\n","        \"\"\"\n","        Given state (of the gridworld), return an action (indexed between 1 and 5) to take.\n","        \"\"\"\n","        state = torch.flatten(torch.tensor(state))\n","        \n","        #action = np.random.randint(5)\n","\n","        prob = np.random.uniform(0, 1)\n","        q_values = self.net(state)\n","        q_values = q_values.detach().numpy()\n","\n","        if (prob < self.eps):\n","            action = np.random.randint(5)\n","\n","        else:\n","            action = np.argmax(q_values)\n","            #action = np.random.randint(5)\n","\n","        if (self.eps > self.eps_min):\n","            self.eps *= self.eps_decay\n","\n","        return action ### TODO EXERCISE 2: IMPLEMENT EPSILON GREEDY INSTEAD OF THE ABOVE RANDOM CHOICE\n","\n","    def update_net(self):\n","        \"\"\"Update the network\n","        \n","        It is trained to predict the value of the whole future for an\n","        action taken given a state:\n","            Q(state, action).\n","\n","        The *actual* information used in the update is the 'reward',\n","        the 'next state' reached, and an 'action' taken from the\n","        previous 'state'.\n","\n","        This is added to the discounted reward for the maximum predicted\n","        Q value of the next state:\n","            max_a [ Q(next_state, a) ]\n","        \"\"\"\n","        self.opt.zero_grad()\n","\n","        sample_indices = np.random.choice(len(self.history), self.batch_size)\n","        sample = [self.history[i] for i in sample_indices]\n","        states, actions, rewards, next_states, dones = tuple(zip(*sample))\n","        states = torch.stack(states)\n","        next_states = torch.stack(next_states)\n","        actions = torch.tensor(actions)\n","        rewards = torch.tensor(rewards)\n","\n","        # What is the value of the whole future, from the next state?\n","        future_q = torch.max(self.net(next_states), axis=1)[0]\n","        future_q = torch.where(torch.tensor(dones), torch.zeros_like(future_q), future_q)\n","\n","        # The target, formed of the real reward + prediction for the next state\n","        q_targets = rewards + self.gamma * future_q\n","\n","        # What the network predicts currently\n","        q_predictions_all_acts = self.net(states)\n","\n","        # sampled at the actions that were taken to obtain the above \n","        # rewards, next states\n","        indices =\\\n","            torch.tensor(list(range(self.batch_size))) * q_predictions_all_acts.shape[1]\\\n","            + actions\n","\n","        q_predictions_all_acts = torch.flatten(q_predictions_all_acts)\n","        q_predictions = torch.gather(\n","          q_predictions_all_acts,\n","          0, \n","          indices\n","        )\n","\n","        # get the MSE loss\n","        loss = torch.mean((q_targets - q_predictions) ** 2)\n","        self.loss_history.append(loss.detach().clone().item())\n","\n","        # run gradient descent!\n","        loss.backward()\n","        self.opt.step()"]},{"cell_type":"markdown","source":["For now, the agent just takes random actions: test that this is what occurs:"],"metadata":{"id":"jzauqZyS6vGh"}},{"cell_type":"code","source":["def agent_play(env = get_new_env(), agent = Agent(), print_things = True):\n","    time_step = env.reset()\n","    \n","    observation = time_step.observation[\"board\"]\n","\n","    while True:\n","        if print_things: print_game(convert_board_num2str(observation))\n","\n","        if time_step.step_type == environment.StepType.LAST:\n","            if path is None: print(\"GAME OVER\")\n","            break\n","\n","        else:\n","            current_state = time_step.observation[\"board\"]\n","            user_input = valid_actions[agent.act(current_state)][0]\n","            if print_things: print(f\"The agent chooses input {user_input}\")\n","        # validate input\n","        if user_input not in valid_action_strings:\n","            print(\"Try an action in\", valid_action_strings.keys())\n","            continue\n","        elif user_input == \"q\":\n","            break\n","\n","        # env takes an integer action - convert input\n","        action = valid_action_strings[user_input]\n","\n","        # take the action in the environment\n","        time_step = env.step(action)\n","        observation = time_step.observation[\"board\"]\n","\n","sample_env = get_new_env(level=0)\n","# sample_env._max_iterations = 200 # you can increase the maximum number of iterations allowed in a given environment\n","agent_play(env = sample_env, agent=Agent(), print_things=True)\n","# if sample_env._episode_return != -100: print(sample_env._episode_return)\n","print(f\"Agent's final reward: {sample_env._episode_return}\")"],"metadata":{"id":"N_mrfORA3J-M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In RL, in order to balance agents both needing to *explore* their environment, as well as *exploit* strategies that lead to high reward, the actions that they take are generally determined by the *epsilon-greedy* rule:\n","\n","* with probability $\\varepsilon$, take a random action.\n","* otherwise, take the action determined by the current policy (in this case, what the neural net predicts is optimal).\n","\n","In most RL implementations, we want $\\varepsilon$ to initially be large, and decrease to a smaller value. \n","\n","Following <a href=\"https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\">common implementations</a>, we use the following $\\varepsilon$ update rule, determine by the `Agent`'s `eps`, `eps_min` and `eps_decay` parameters:\n","\n","* *after* every time the agent acts, turn its current `eps` parameter into `eps * eps_decay` unless `eps < eps_min`, in which case keep it constant\n","\n","**Exercise 2**: In the `Agent` class, implement the `act` function to takes actions based on an $\\varepsilon$-greedy approach with decay factor. \n","\n","<details>\n","<summary>Hint:</summary>\n","`net.predict(state)` should return the Q values the neural net predicts for each of the actions.\n","</details>\n","\n","Note: testing randomised implementations is not an easy feat, so the tests may pass with an imperfect implementation. "],"metadata":{"id":"UZ4HK8XQ8noN"}},{"cell_type":"code","source":["### TESTS FOR EXERCISE 2\n","\n","test2_env = get_new_env()\n","test2_agent = Agent()\n","start_state = test2_env.reset()\n","NO_TESTS_2 = 1000\n","action_dist = [0 for i in range(5)]\n","\n","print(\"Running tests...\", end=\"\")\n","for TEST in tqdm(range(NO_TESTS_2)):\n","    \n","    action = test2_agent.act(start_state.observation[\"board\"])\n","    assert action in list(range(5)), f\"Action not in {list(range(5))}\"\n","    action_dist[action] += 1\n","\n","for i in range(5):\n","    if action_dist[i] == max(action_dist):\n","        assert 650 <= action_dist[i] <= 800, f\"It doesn't look as if your epsilon-greedy implementation selects the optimal action enough of the time.\"\n","    else:\n","        assert 30 <= action_dist[i] <= 120, \"It looks like your epsilon-greedy implementation isn't selecting random actions correctly.\"\n","print(\" ... tests passed!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7aHaoe-G-l5P","executionInfo":{"status":"ok","timestamp":1647709298892,"user_tz":0,"elapsed":488,"user":{"displayName":"Daniel Nicolae","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11863936580723690012"}},"outputId":"aebaf082-436e-4f42-e938-a5ef5a8d4400"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Running tests..."]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 4252.09it/s]"]},{"output_type":"stream","name":"stdout","text":[" ... tests passed!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"MrkuN4gc7B79"},"source":["## Learning\n","\n","We now get to the interesting part: learning!\n","\n","# Q-learning (skip if you know what Q-learning is!)\n","\n","The RL algorithm we'll start with is *Q-learning*. Recall that in the explanation of $\\varepsilon $-greedy approaches above, the agent followed which action it thought was optimal according to its policy. In Q-learning, the neural network models \n","\n","*the agent's current prediction of the reward it expects to obtain given that it is in a state $s$ and it will take the action $a$ next.*\n","\n","We call this value $Q(s, a)$. The idea of Q-learning is to store all the past actions that the agent made and the rewards it received in the episode following that, and use that to update the agent's model of $Q(s, a)$ by gradient descent. \n","\n","# Implementation\n","\n","We've implemented most of the agent, namely the storage of past experiences as well as part of the `learn` function. Using what you implemented earlier in getting rewards that the agent gets per step, implement the assignment to variables `action` and `step_reward` in the below code.\n","\n","Then get training! You should be able to then use the `agent_play` function in order to see what your agent is doing.\n","\n","WARNING: RL algorithms are notoriously hard to debug, read <a href=\"https://andyljones.com/posts/rl-debugging.html\">this</a>. One of the biggest reasons being that it can often take a long time for them to do even anything even slightly better than random. After training for 200 steps on level 2, here were our episode rewards (after increasing `env._max_iterations` to 200).\n","\n","<details>\n","<summary>Spoiler</summary>\n","Agent's final reward: -200\n","Agent's final reward: -25\n","Agent's final reward: -200\n","Agent's final reward: 14\n","Agent's final reward: -97\n","Agent's final reward: -64\n","Agent's final reward: -132\n","Agent's final reward: -122\n","Agent's final reward: -200\n","Agent's final reward: -6\n","Agent's final reward: 16\n","Agent's final reward: -11\n","Agent's final reward: -2\n","Agent's final reward: 27\n","Agent's final reward: 17\n","Agent's final reward: -7\n","Agent's final reward: 11\n","Agent's final reward: -61\n","Agent's final reward: -135\n","Agent's final reward: -200\n","Agent's final reward: -130\n","Agent's final reward: 8\n","Agent's final reward: -200\n","Agent's final reward: 8\n","Agent's final reward: -54\n","Agent's final reward: -200\n","Agent's final reward: 2\n","Agent's final reward: -21\n","Agent's final reward: -61\n","Agent's final reward: -46\n","Agent's final reward: -200\n","Agent's final reward: -17\n","Agent's final reward: -28\n","Agent's final reward: 36\n","Agent's final reward: -78\n","Agent's final reward: -142\n","Agent's final reward: -5\n","Agent's final reward: -200\n","Agent's final reward: 31\n","Agent's final reward: -107\n","Agent's final reward: -90\n","Agent's final reward: -17\n","Agent's final reward: -31\n","Agent's final reward: -63\n","Agent's final reward: -141\n","Agent's final reward: -68\n","Agent's final reward: 0\n","Agent's final reward: 30\n","Agent's final reward: -200\n","Agent's final reward: -14\n","Agent's final reward: 21\n","Agent's final reward: -200\n","Agent's final reward: -40\n","Agent's final reward: 2\n","Agent's final reward: -70\n","Agent's final reward: -200\n","Agent's final reward: -21\n","Agent's final reward: -200\n","Agent's final reward: -200\n","Agent's final reward: 29\n","Agent's final reward: -110\n","Agent's final reward: -200\n","Agent's final reward: 4\n","Agent's final reward: -27\n","Agent's final reward: -62\n","Agent's final reward: -11\n","Agent's final reward: -200\n","Agent's final reward: -39\n","Agent's final reward: -112\n","Agent's final reward: -87\n","Agent's final reward: -75\n","Agent's final reward: -48\n","Agent's final reward: -200\n","Agent's final reward: -72\n","Agent's final reward: -200\n","Agent's final reward: -6\n","Agent's final reward: -8\n","Agent's final reward: -200\n","Agent's final reward: -1\n","Agent's final reward: -141\n","Agent's final reward: -200\n","Agent's final reward: -35\n","Agent's final reward: -200\n","Agent's final reward: 23\n","Agent's final reward: -60\n","Agent's final reward: 17\n","Agent's final reward: -57\n","Agent's final reward: -95\n","Agent's final reward: -21\n","Agent's final reward: -109\n","Agent's final reward: -200\n","Agent's final reward: -200\n","Agent's final reward: -200\n","Agent's final reward: -200\n","Agent's final reward: -40\n","Agent's final reward: -4\n","Agent's final reward: -200\n","Agent's final reward: -200\n","Agent's final reward: 23\n","Agent's final reward: 34\n","</details> "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iXMI2ZPN7Dpk"},"outputs":[],"source":["def learn(\n","    env, \n","    agent, \n","    max_episodes=100, \n","    verbose=True,\n","):\n","    \"\"\"\n","    A generic solve function (for experience replay agents).\n","    Inheriting environments should implement this.\n","    Uses overridden functions to customise the behaviour.\n","    \"\"\"\n","    start_time = datetime.datetime.now()\n","    all_episode_scores = []\n","    all_episode_lengths = []\n","\n","    for episode in range(max_episodes):\n","        # Initialise the environment state\n","        total_ep_reward = 0\n","        current_time_step = env.reset()\n","\n","        # Take steps until failure / win\n","        for t in itertools.count():\n","        \n","            ## TODO EXERCISE 3 \n","            old_state = None # should be a tensor of length 42 (the size of the grid)\n","            action = None # should be an integer in [0, 1, 2, 3, 4]\n","            step_reward =  None # should be an integer\n","            new_state = None # same format as old_state\n","            episode_done = None # should be a bool\n","            ## END EXERCISE 3\n","\n","            total_ep_reward += step_reward\n","\n","            agent.history.append((\n","                old_state,\n","                action,\n","                step_reward,\n","                new_state,\n","                episode_done,\n","            ))\n","\n","            # forget old experiences\n","            if len(agent.history) > agent.history_size:\n","              agent.history.popleft()\n","\n","            # one of the most important lines! this does gradient descent\n","            agent.update_net() \n","\n","            if verbose:\n","                print(\n","                    f\"\\rStep {t} ({len(agent.history)}) \"\n","                    f\"@ Episode {episode + 1}/{max_episodes}, \"\n","                    f\"rwd {total_ep_reward}\",\n","                    # f\"loss: {loss}\",\n","                    end=\"\")\n","                sys.stdout.flush()\n","\n","            if episode_done:\n","                break\n","\n","        # HANDLE EPISODE END\n","        print(total_ep_reward)\n","        all_episode_lengths.append(t)\n","        all_episode_scores.append(total_ep_reward)\n","        all_episode_scores.append(total_ep_reward)\n","\n","    print(\"\\nTIME ELAPSED\", datetime.datetime.now() - start_time)\n","\n","    return all_episode_scores, all_episode_lengths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OpBxuOeEwV6t"},"outputs":[],"source":["agent = Agent(batch_size=128)\n","all_episode_scores, all_episode_lengths = learn(get_new_env(level=0), agent, max_episodes=1000)"]},{"cell_type":"markdown","source":["You should at least see decreasing losses in your network's Q values predictions:"],"metadata":{"id":"f7CANvgGNBvi"}},{"cell_type":"code","source":["plt.scatter(list(range(len(agent.loss_history))), agent.loss_history)"],"metadata":{"id":"Z7IKpPgM39qM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nABPkzC38tw8"},"source":["On level 0, does the agent learn to avoid the interrupt tile or not? \n","How reliable is this?\n","\n","Once you've got an agent working on the simplest grid world, it might be a good idea to optimize our implementation to get better learning first!\n","\n","Then, start doing some work on the Interruptibility problem: see if you can make progress - does the agent learn to press the interrupt button or not? Does this depend on the Q learning implementation?\n","\n","# Extension - make SARSA Safely Interruptible\n","\n","The original Safely Interruptible paper mentions that the SARSA algorithm is not safely interruptible, but can be modified to be so. There aren't any implementations of this available at the moment though - be the first!"]}],"metadata":{"accelerator":"GPU","colab":{"name":"Copy of AI Safety Workshop.ipynb","provenance":[{"file_id":"1Yfk1a4EkCEEddzW-iNfIaH_8-yPK3Ddo","timestamp":1647704439433},{"file_id":"12zrj92Rs1UIp1NnQV8RCXDYyxZywOh7L","timestamp":1647453595956}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}