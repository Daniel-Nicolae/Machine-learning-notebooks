{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of CuAI NLP Workshop #2 - Transformers and Attention.ipynb","provenance":[{"file_id":"16jJPkSECLhuL2ghldTwUShnMFWldbXF9","timestamp":1647710119680}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"fe46139608f846628b1b0e843e08e6f7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_75d6cb6e96c340e0b77212330ec7fe31","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7a1e8fae94364500b953834e093b7dfd","IPY_MODEL_230e651b40c4429a9e11c893ed87fb5c","IPY_MODEL_f0793f83e4f7454bacdf5dc802b34aac"]}},"75d6cb6e96c340e0b77212330ec7fe31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7a1e8fae94364500b953834e093b7dfd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3cf1b1600b7648eca454d48d65c5821a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"pos files: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e0ea9f6e724744d09890e1c87c66db30"}},"230e651b40c4429a9e11c893ed87fb5c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3fd46c6af9cf4b3db438ab23379b027e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":12500,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":12500,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6641c678b7ea4918859d3f7dc773258e"}},"f0793f83e4f7454bacdf5dc802b34aac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_84754f70dfd04300b8e3882c71ef09ee","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 12500/12500 [00:00&lt;00:00, 15088.91it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_14d85a327e614285a75e6b8493e57d0b"}},"3cf1b1600b7648eca454d48d65c5821a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e0ea9f6e724744d09890e1c87c66db30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3fd46c6af9cf4b3db438ab23379b027e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6641c678b7ea4918859d3f7dc773258e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"84754f70dfd04300b8e3882c71ef09ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"14d85a327e614285a75e6b8493e57d0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9e7e42a0867a414d81f0d0aea78ae7ba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a55a5b42d9694b9fbf765c150a326c64","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_47f51313641145bd94975c0293bc46ac","IPY_MODEL_f5391dd60b28497a98af13755d765b13","IPY_MODEL_76ae61dfe9fb4fb6986f0bed8951d31e"]}},"a55a5b42d9694b9fbf765c150a326c64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"47f51313641145bd94975c0293bc46ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d7f41d337b7f4056961da19f0fa0f394","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"neg files: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_45159213e8cd4a469a949e6a2966a698"}},"f5391dd60b28497a98af13755d765b13":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2a1013c44c1a46d291f9ebb84f543890","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":12500,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":12500,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f49fa50fbc924f92afbcfa41b73222d6"}},"76ae61dfe9fb4fb6986f0bed8951d31e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_871a8b13429f4a70adb704118049b631","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 12500/12500 [00:00&lt;00:00, 15621.57it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6e367066f3bb4559afbdaca4f573cf87"}},"d7f41d337b7f4056961da19f0fa0f394":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"45159213e8cd4a469a949e6a2966a698":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2a1013c44c1a46d291f9ebb84f543890":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f49fa50fbc924f92afbcfa41b73222d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"871a8b13429f4a70adb704118049b631":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6e367066f3bb4559afbdaca4f573cf87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5d1a18abbf5a48c7ba45e3829fcb5397":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9e30e986755c42bdacd165ee75aa1f14","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cb9d79a977f448a9a025a660b836776f","IPY_MODEL_62a4a040a689480e89a3976f627c52e5","IPY_MODEL_d31e2ac6042347b188a217b628ce8716"]}},"9e30e986755c42bdacd165ee75aa1f14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cb9d79a977f448a9a025a660b836776f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9c1d2c6f0cda44d6803d3c3c3c772de7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"pos files: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f2ba4e731e3849a3a6633b072c13d26f"}},"62a4a040a689480e89a3976f627c52e5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3957b00fb6a2451c818c7c9d795693b3","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":12500,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":12500,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0898c83f0f3c4d1e8d655e1fffc582e4"}},"d31e2ac6042347b188a217b628ce8716":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_66451d63901b43c4a6c8e776fe6febee","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 12500/12500 [00:00&lt;00:00, 13461.04it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e837ed8e8c05450791b80905f4cbf4f2"}},"9c1d2c6f0cda44d6803d3c3c3c772de7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f2ba4e731e3849a3a6633b072c13d26f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3957b00fb6a2451c818c7c9d795693b3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0898c83f0f3c4d1e8d655e1fffc582e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"66451d63901b43c4a6c8e776fe6febee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e837ed8e8c05450791b80905f4cbf4f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"abb086b26fa8410fa367ab131bb1aeb7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2bd477c9e8be453e99c7eec507164028","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fc15c057bcbd4f8a9d38598c823f4333","IPY_MODEL_c49db9f8760648b0bd60f8e3f97805b4","IPY_MODEL_e081fd4e42774436a5dae302716603c9"]}},"2bd477c9e8be453e99c7eec507164028":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fc15c057bcbd4f8a9d38598c823f4333":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_20aeed83946a49aa9156f7b5a5a72280","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"neg files: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_41e69f6c433f40c6b976cb7e5739ac21"}},"c49db9f8760648b0bd60f8e3f97805b4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ba7dd87ad8314da6b82ecb2ef8df7b2d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":12500,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":12500,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3830f7fca2c042bb93b9dc8aa51c1b24"}},"e081fd4e42774436a5dae302716603c9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d2e43be050c549e58ecb46f19e4e9c74","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 12500/12500 [00:00&lt;00:00, 13489.08it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_98982e34e43949eaa22e37fe9858bf89"}},"20aeed83946a49aa9156f7b5a5a72280":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"41e69f6c433f40c6b976cb7e5739ac21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ba7dd87ad8314da6b82ecb2ef8df7b2d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3830f7fca2c042bb93b9dc8aa51c1b24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d2e43be050c549e58ecb46f19e4e9c74":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"98982e34e43949eaa22e37fe9858bf89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e826cf3e6222413dbf927e7107e866ed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_861743527f884356aa8efe331af3fe97","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_93fd28292bce400b947fa5366563e8e8","IPY_MODEL_56affae1019f4594a19d0ec4f19fd531","IPY_MODEL_28e8f7aa2b56407dbc40ba4b3f11c2d3"]}},"861743527f884356aa8efe331af3fe97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"93fd28292bce400b947fa5366563e8e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_461cbd294a5c4d6692a7a4e75e43d222","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_17d65adb6c1c4904b26c6ee240b6c57a"}},"56affae1019f4594a19d0ec4f19fd531":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_961e86f1015141e8b9528bb0ebf6fce1","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"danger","max":3,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":3,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d93cb31dcf484f48ab1ce8336059b0a0"}},"28e8f7aa2b56407dbc40ba4b3f11c2d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_79242855436748468d5a38718e911dae","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3/3 [00:01&lt;00:00,  2.13it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ea997f91690146cfa6910b57a97f9d10"}},"461cbd294a5c4d6692a7a4e75e43d222":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"17d65adb6c1c4904b26c6ee240b6c57a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"961e86f1015141e8b9528bb0ebf6fce1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d93cb31dcf484f48ab1ce8336059b0a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"79242855436748468d5a38718e911dae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ea997f91690146cfa6910b57a97f9d10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9317552c257e4e49beafb2a5db4ec1fe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f33c8aeb809f4a47b0a296dc97f99533","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1f11c8beddf347abac9da8948fbf5196","IPY_MODEL_f461c1dcc3ec436e98e5df2251456732","IPY_MODEL_5289c9f9a64240bab960873c79b02ef8"]}},"f33c8aeb809f4a47b0a296dc97f99533":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1f11c8beddf347abac9da8948fbf5196":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b07423db200d41369ed664ee9085ba17","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0e3bc1e131344005bd07dcb4ba13aa2c"}},"f461c1dcc3ec436e98e5df2251456732":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f124be6d460e476bbbb323bc4ebde926","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"danger","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_125da5a95d854eb7b97b5ba649910de5"}},"5289c9f9a64240bab960873c79b02ef8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_61b06af9785547f2b5df62668cf09e92","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/? [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fa87c4cbc63440d9824f8e8141d6e826"}},"b07423db200d41369ed664ee9085ba17":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0e3bc1e131344005bd07dcb4ba13aa2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f124be6d460e476bbbb323bc4ebde926":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"125da5a95d854eb7b97b5ba649910de5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":"20px","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"61b06af9785547f2b5df62668cf09e92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fa87c4cbc63440d9824f8e8141d6e826":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["FEEDBACK: https://forms.gle/FvmxWWisFqCmsoDf9\n","\n","*permalink: https://tinyurl.com/CuAINLP2*\n","\n","BERT attention visualisation: https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ\n","\n","If you want to do a task more similar to what we did last week, we recommend the Stanford CS224N exercise here: https://web.stanford.edu/class/cs224n/assignments/a5.zip , instructions here: https://web.stanford.edu/class/cs224n/assignments/a5.pdf (requires running Python locally).\n","\n","Change Pytorch classes: https://colab.research.google.com/drive/1dQVo-krTVoj0b9ekJNdGAd0lirjUQV70#scrollTo=MdYXZvhrGLIW\n","\n","# Introduction to Transformers\n","\n"],"metadata":{"id":"PLOix78-sdJG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"p5qZez2loF8V"},"outputs":[],"source":["%pip install transformers\n","%pip install sentencepiece ## 16 seconds\n","import torch as t\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["## Machine Translation"],"metadata":{"id":"eQaOreeBw7vI"}},{"cell_type":"code","source":["import sentencepiece\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n","translator = pipeline(\"translation_en_to_de\")\n","translation_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-nl\") \n","translation_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-nl\") ## 1 minute"],"metadata":{"id":"DVmYSg8atMZl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["translation_text = \"Welcome to our natural language processing workshop\"\n","tokenized_text = translation_tokenizer.prepare_seq2seq_batch([translation_text])\n","translation_input_ids = t.tensor(tokenized_text['input_ids']).long()\n","translation = translation_model.generate(input_ids = translation_input_ids)\n","translated_text = translation_tokenizer.batch_decode(translation, skip_special_tokens=True)[0]\n","print('-' * 50)\n","print(f\"ORIGINAL: {translation_text}\")\n","print(f\"TRANSLATION: {translated_text}\")"],"metadata":{"id":"0BPNQZsLuGT9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644674665379,"user_tz":0,"elapsed":673,"user":{"displayName":"Arthur Conmy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkoRJSBDElT-ypT7GdBmkdRp5s6AtU2rcBsGV71w=s64","userId":"06695409732812757016"}},"outputId":"3e4986bb-2eef-4acf-f34a-c3f1c26d69ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:3478: FutureWarning: \n","`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n","`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n","your targets.\n","\n","Here is a short example:\n","\n","model_inputs = tokenizer(src_texts, ...)\n","with tokenizer.as_target_tokenizer():\n","    labels = tokenizer(tgt_texts, ...)\n","model_inputs[\"labels\"] = labels[\"input_ids\"]\n","\n","See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n","For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n","\n","  warnings.warn(formatted_warning, FutureWarning)\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","ORIGINAL: Welcome to our natural language processing workshop\n","TRANSLATION: Welkom op onze workshop natuurlijke taalverwerking\n"]}]},{"cell_type":"code","source":["translation_model"],"metadata":{"id":"jT9OVrI_vgdl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Language Modelling"],"metadata":{"id":"Ne9l3-7px1sV"}},{"cell_type":"code","source":["# from transformers import AutoModelForCausalLM\n","# language_model_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n","# language_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\") # a better model, but colab fails loading it -_-\n","\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')"],"metadata":{"id":"ih8vWK6LyN9F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = gpt2_tokenizer(\"\"\"def fib(n):\\n # implement Fibonacci numbers\"\"\", return_tensors=\"pt\")[\"input_ids\"]\n","\n","gen_len = 50\n","batch_size = 5\n","\n","samples = gpt2_model.generate(\n","    inputs, \n","    max_length=inputs.shape[-1]+gen_len, \n","    min_length=inputs.shape[-1]+gen_len, \n","    do_sample=True, \n","    temperature=0.6, \n","    top_k=len(gpt2_tokenizer), \n","    top_p=1.0, \n","    num_return_sequences=batch_size, \n","    use_cache=True\n",")\n","\n","for sample_no in range(1, batch_size + 1):\n","    print('-' * 50)\n","    print(f\"Completion {sample_no} of {batch_size}:\")\n","    print(gpt2_tokenizer.decode(samples[sample_no - 1])) ## this cell takes "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXpGLpRTfrpV","executionInfo":{"status":"ok","timestamp":1644675945509,"user_tz":0,"elapsed":8181,"user":{"displayName":"Arthur Conmy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkoRJSBDElT-ypT7GdBmkdRp5s6AtU2rcBsGV71w=s64","userId":"06695409732812757016"}},"outputId":"44aca530-996b-42f6-c940-da77108b0d5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","Completion 1 of 5:\n","def fib(n):\n"," # implement Fibonacci numbers\n","\n","for i in range(100): print (i[i]) # output fib(i[1]) fib(1) # output fib(1) fib(1)\n","\n","And then, using the above method, we can define\n","--------------------------------------------------\n","Completion 2 of 5:\n","def fib(n):\n"," # implement Fibonacci numbers\n","\n","#\n","\n","# Note: It is possible to use fib() to return a number in a larger unit.\n","\n","#\n","\n","# The Fibonacci number is the sum of the sum of the\n","\n","# integers in a given\n","--------------------------------------------------\n","Completion 3 of 5:\n","def fib(n):\n"," # implement Fibonacci numbers.\n","\n","return fib(n + 1, fib(n))\n","\n","def fib(n):\n","\n","# implement Fibonacci numbers.\n","\n","return fib(n + 1, fib(n))\n","\n","def fib_index(\n","--------------------------------------------------\n","Completion 4 of 5:\n","def fib(n):\n"," # implement Fibonacci numbers # for i in range(n): # for i in range(n) % fib(n) def fib(n): return fib(n)\n","\n","This code is far from being complete. I have already written some functions to get the\n","--------------------------------------------------\n","Completion 5 of 5:\n","def fib(n):\n"," # implement Fibonacci numbers\n","\n","#...\n","\n","#...\n","\n","#...\n","\n","#...\n","\n","#...\n","\n","#...\n","\n","#...\n","\n","#...\n","\n","#...\n","\n","#...\n","\n","#...\n","\n","#...\n","\n","\n"]}]},{"cell_type":"markdown","source":["## Vision Transformer"],"metadata":{"id":"x7SCMhDv4p-H"}},{"cell_type":"markdown","source":["Source: <a href=\"https://www.facebook.com/cambridge.university/photos/a.10150161669754864/10159007779694864/\">University of Cambridge Facebook</a>, Jan 2022.\n","<img src=\"https://i.imgur.com/ChLjNFc.png\">\n","\n","Model: https://huggingface.co/microsoft/beit-base-patch16-224-pt22k-ft22k\n"],"metadata":{"id":"oWi6F1nH3QpV"}},{"cell_type":"markdown","source":["## What is a Transformer?"],"metadata":{"id":"yhPz76D64sk3"}},{"cell_type":"code","source":["print(gpt2_model.transformer.h[0].attn.c_attn._parameters['weight'].shape)\n","gpt2_model\n","\n","    # (9): GPT2Block(\n","    # (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    # (attn): GPT2Attention(\n","    #     (c_attn): Conv1D()\n","    #     (c_proj): Conv1D()\n","    #     (attn_dropout): Dropout(p=0.1, inplace=False)\n","    #     (resid_dropout): Dropout(p=0.1, inplace=False)\n","    # )\n","    # (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    # (mlp): GPT2MLP(\n","    #     (c_fc): Conv1D()\n","    #     (c_proj): Conv1D()\n","    #     (dropout): Dropout(p=0.1, inplace=False)\n","    # )\n","    # )"],"metadata":{"id":"NDcSWHSfrrcu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644679289830,"user_tz":0,"elapsed":207,"user":{"displayName":"Arthur Conmy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkoRJSBDElT-ypT7GdBmkdRp5s6AtU2rcBsGV71w=s64","userId":"06695409732812757016"}},"outputId":"7152804f-e07e-4ddc-b96e-50b7b7ffb3e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([768, 2304])\n"]},{"output_type":"execute_result","data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["* Just an abstraction for a composition of many functions (mostly matrix multiplication)?\n","\n","* Composed of *blocks* that stack not only deeply (like powerful image models), but wide, too.\n","\n","![](https://jalammar.github.io/images/t/Transformer_encoder.png)\n","\n","* Confusion alert: interpret 'encoder' to mean 'transformation of word embeddings', however in the context of translation, there are separate encoding and decoding parts of a transformer model (though very similar):\n","\n","![](https://jalammar.github.io/images/t/Transformer_decoder.png)\n","\n","* More detail on a block:\n","\n","![](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png)\n","\n","* A vision transformer works very similarly https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html :\n","\n","![](https://1.bp.blogspot.com/-_mnVfmzvJWc/X8gMzhZ7SkI/AAAAAAAAG24/8gW2AHEoqUQrBwOqjhYB37A7OOjNyKuNgCLcBGAsYHQ/s16000/image1.gif)\n","\n","* A language model produces encodings for all text it has processed. We only use the last encoding for language modelling, but it is very useful to use the other encodings for other tasks (see below)."],"metadata":{"id":"3EiQXejRpNUG"}},{"cell_type":"markdown","source":["## Using and finetuning a pretrained model\n","\n","In this extension section, we will provide code to use a pretrained model in order to do another task upstream: sentiment classification!\n","\n","Some code is borrowed from https://gmihaila.github.io/tutorial_notebooks/gpt2_finetune_classification/ (which is apache licensed :) ).\n","\n","Download the dataset:"],"metadata":{"id":"NjPDp_UutWxD"}},{"cell_type":"code","source":["!wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -zxf /content/aclImdb_v1.tar.gz"],"metadata":{"id":"ndBqegMlta7c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Boring setup:\n","\n","No need to understand anything from the below cell."],"metadata":{"id":"kTke1jJMpWt2"}},{"cell_type":"code","source":["import io\n","import os\n","import torch\n","from tqdm.notebook import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","# from ml_things import plot_dict, plot_confusion_matrix, fix_text\n","from sklearn.metrics import classification_report, accuracy_score\n","from transformers import (set_seed,\n","                          TrainingArguments,\n","                          Trainer,\n","                          GPT2Config,\n","                          GPT2Tokenizer,\n","                          AdamW, \n","                          get_linear_schedule_with_warmup,\n","                          GPT2ForSequenceClassification)\n","\n","class Gpt2ClassificationCollator(object):\n","    r\"\"\"\n","    Data Collator used for GPT2 in a classificaiton rask. \n","    \n","    It uses a given tokenizer and label encoder to convert any text and labels to numbers that \n","    can go straight into a GPT2 model.\n","\n","    This class is built with reusability in mind: it can be used as is as long\n","    as the `dataloader` outputs a batch in dictionary format that can be passed \n","    straight into the model - `model(**batch)`.\n","\n","    Arguments:\n","\n","      use_tokenizer (:obj:`transformers.tokenization_?`):\n","          Transformer type tokenizer used to process raw text into numbers.\n","\n","      labels_ids (:obj:`dict`):\n","          Dictionary to encode any labels names into numbers. Keys map to \n","          labels names and Values map to number associated to those labels.\n","\n","      max_sequence_len (:obj:`int`, `optional`)\n","          Value to indicate the maximum desired sequence to truncate or pad text\n","          sequences. If no value is passed it will used maximum sequence size\n","          supported by the tokenizer and model.\n","\n","    \"\"\"\n","\n","    def __init__(self, use_tokenizer, labels_encoder, max_sequence_len=None):\n","\n","        # Tokenizer to be used inside the class.\n","        self.use_tokenizer = use_tokenizer\n","        # Check max sequence length.\n","        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n","        # Label encoder used inside the class.\n","        self.labels_encoder = labels_encoder\n","\n","        return\n","\n","    def __call__(self, sequences):\n","        r\"\"\"\n","        This function allowes the class objesct to be used as a function call.\n","        Sine the PyTorch DataLoader needs a collator function, I can use this \n","        class as a function.\n","\n","        Arguments:\n","\n","          item (:obj:`list`):\n","              List of texts and labels.\n","\n","        Returns:\n","          :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model.\n","          It holddes the statement `model(**Returned Dictionary)`.\n","        \"\"\"\n","\n","        # Get all texts from sequences list.\n","        texts = [sequence['text'] for sequence in sequences]\n","        # Get all labels from sequences list.\n","        labels = [sequence['label'] for sequence in sequences]\n","        # Encode all labels using label encoder.\n","        labels = [self.labels_encoder[label] for label in labels]\n","        # Call tokenizer on all texts to convert into tensors of numbers with \n","        # appropriate padding.\n","        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n","        # Update the inputs with the associated encoded labels as tensor.\n","        inputs.update({'labels':torch.tensor(labels)})\n","\n","        return inputs\n","\n","# Set seed for reproducibility.\n","set_seed(123)\n","\n","# Number of training epochs (authors on fine-tuning Bert recommend between 2 and 4).\n","epochs = 4\n","\n","# Number of batches - depending on the max sequence length and GPU memory.\n","# For 512 sequence length batch of 10 works without cuda memory issues.\n","# For small sequence length can try batch of 32 or higher.\n","batch_size = 32\n","\n","# Pad or truncate text sequences to a specific length\n","# if `None` it will use maximum sequence of word piece tokens allowed by model.\n","max_length = 60\n","\n","# Look for gpu to use. Will use `cpu` by default if no gpu found.\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Name of transformers model - will use already pretrained model.\n","# Path of transformer model - will load your own model from local disk.\n","model_name_or_path = 'gpt2'\n","\n","# Dictionary of labels and their id - this will be used to convert.\n","# String labels to number ids.\n","labels_ids = {'neg': 0, 'pos': 1}\n","\n","# How many labels are we using in training.\n","# This is used to decide size of classification head.\n","n_labels = len(labels_ids)\n","\n","class MovieReviewsDataset(Dataset):\n","  r\"\"\"PyTorch Dataset class for loading data.\n","\n","  This is where the data parsing happens.\n","\n","  This class is built with reusability in mind: it can be used as is as.\n","\n","  Arguments:\n","\n","    path (:obj:`str`):\n","        Path to the data partition.\n","\n","  \"\"\"\n","\n","  def __init__(self, path, use_tokenizer):\n","\n","    # Check if path exists.\n","    if not os.path.isdir(path):\n","      # Raise error if path is invalid.\n","      raise ValueError('Invalid `path` variable! Needs to be a directory')\n","    self.texts = []\n","    self.labels = []\n","    # Since the labels are defined by folders with data we loop \n","    # through each label.\n","    for label in ['pos', 'neg']:\n","      sentiment_path = os.path.join(path, label)\n","\n","      # Get all files from path.\n","      files_names = os.listdir(sentiment_path)#[:10] # Sample for debugging.\n","      # Go through each file and read its content.\n","      for file_name in tqdm(files_names, desc=f'{label} files'):\n","        file_path = os.path.join(sentiment_path, file_name)\n","\n","        # Read content.\n","        content = io.open(file_path, mode='r', encoding='utf-8').read()\n","        # Fix any unicode issues.\n","        # content = fix_text(content)\n","        # Save content.\n","        self.texts.append(content)\n","        # Save encode labels.\n","        self.labels.append(label)\n","\n","    # Number of exmaples.\n","    self.n_examples = len(self.labels)\n","    \n","\n","    return\n","\n","  def __len__(self):\n","    r\"\"\"When used `len` return the number of examples.\n","\n","    \"\"\"\n","    \n","    return self.n_examples\n","\n","  def __getitem__(self, item):\n","    r\"\"\"Given an index return an example from the position.\n","    \n","    Arguments:\n","\n","      item (:obj:`int`):\n","          Index position to pick an example to return.\n","\n","    Returns:\n","      :obj:`Dict[str, str]`: Dictionary of inputs that contain text and \n","      asociated labels.\n","\n","    \"\"\"\n","\n","    return {'text':self.texts[item],\n","            'label':self.labels[item]}\n","\n","def validation(model, dataloader, device_, valid_batches):\n","\n","  model.eval()\n","\n","  correct = 0\n","  incorrect = 0\n","\n","  for batch_idx, batch in tqdm(enumerate(dataloader), total=min(valid_batches, len(dataloader))):\n","    if batch_idx == valid_batches:\n","        break\n","    true_labels = batch['labels']\n","    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","        loss, logits = outputs[:2]\n","        \n","        logits = logits.detach().cpu().numpy()\n","        predict_content = logits.argmax(axis=-1).flatten().tolist()\n","        print(predict_content)\n","\n","        correct += t.sum( (t.tensor(predict_content) == t.tensor(true_labels)).long() )\n","        incorrect += t.sum( (t.tensor(predict_content) != t.tensor(true_labels)).long() )\n","\n","  model.train()  \n","  print(f\"On the validation dataset, the model got {correct} sentiments correct and {incorrect} sentiments incorrect\")\n","  return\n","\n","gpt2_tokenizer.padding_side = \"left\"\n","# Define PAD Token = EOS Token = 50256\n","gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n","\n","gpt2_classificaiton_collator = Gpt2ClassificationCollator(use_tokenizer=gpt2_tokenizer, \n","                                                          labels_encoder=labels_ids, \n","                                                          max_sequence_len=max_length)\n","\n","print('Dealing with Train...')\n","# Create pytorch dataset.\n","train_dataset = MovieReviewsDataset(path='/content/aclImdb/train', \n","                               use_tokenizer=gpt2_tokenizer)\n","print('Created `train_dataset` with %d examples!'%len(train_dataset))\n","\n","# Move pytorch dataset into dataloader.\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)\n","print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n","\n","print()\n","\n","print('Dealing with Validation...')\n","# Create pytorch dataset.\n","valid_dataset =  MovieReviewsDataset(path='/content/aclImdb/test', \n","                               use_tokenizer=gpt2_tokenizer)\n","print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n","\n","# Move pytorch dataset into dataloader.\n","valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)\n","print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":266,"referenced_widgets":["fe46139608f846628b1b0e843e08e6f7","75d6cb6e96c340e0b77212330ec7fe31","7a1e8fae94364500b953834e093b7dfd","230e651b40c4429a9e11c893ed87fb5c","f0793f83e4f7454bacdf5dc802b34aac","3cf1b1600b7648eca454d48d65c5821a","e0ea9f6e724744d09890e1c87c66db30","3fd46c6af9cf4b3db438ab23379b027e","6641c678b7ea4918859d3f7dc773258e","84754f70dfd04300b8e3882c71ef09ee","14d85a327e614285a75e6b8493e57d0b","9e7e42a0867a414d81f0d0aea78ae7ba","a55a5b42d9694b9fbf765c150a326c64","47f51313641145bd94975c0293bc46ac","f5391dd60b28497a98af13755d765b13","76ae61dfe9fb4fb6986f0bed8951d31e","d7f41d337b7f4056961da19f0fa0f394","45159213e8cd4a469a949e6a2966a698","2a1013c44c1a46d291f9ebb84f543890","f49fa50fbc924f92afbcfa41b73222d6","871a8b13429f4a70adb704118049b631","6e367066f3bb4559afbdaca4f573cf87","5d1a18abbf5a48c7ba45e3829fcb5397","9e30e986755c42bdacd165ee75aa1f14","cb9d79a977f448a9a025a660b836776f","62a4a040a689480e89a3976f627c52e5","d31e2ac6042347b188a217b628ce8716","9c1d2c6f0cda44d6803d3c3c3c772de7","f2ba4e731e3849a3a6633b072c13d26f","3957b00fb6a2451c818c7c9d795693b3","0898c83f0f3c4d1e8d655e1fffc582e4","66451d63901b43c4a6c8e776fe6febee","e837ed8e8c05450791b80905f4cbf4f2","abb086b26fa8410fa367ab131bb1aeb7","2bd477c9e8be453e99c7eec507164028","fc15c057bcbd4f8a9d38598c823f4333","c49db9f8760648b0bd60f8e3f97805b4","e081fd4e42774436a5dae302716603c9","20aeed83946a49aa9156f7b5a5a72280","41e69f6c433f40c6b976cb7e5739ac21","ba7dd87ad8314da6b82ecb2ef8df7b2d","3830f7fca2c042bb93b9dc8aa51c1b24","d2e43be050c549e58ecb46f19e4e9c74","98982e34e43949eaa22e37fe9858bf89"]},"id":"Uvbao6_YmK_S","executionInfo":{"status":"ok","timestamp":1644676015764,"user_tz":0,"elapsed":4249,"user":{"displayName":"Arthur Conmy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkoRJSBDElT-ypT7GdBmkdRp5s6AtU2rcBsGV71w=s64","userId":"06695409732812757016"}},"outputId":"27a151db-a1f0-40ef-91bd-c72aaf4963cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dealing with Train...\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe46139608f846628b1b0e843e08e6f7","version_minor":0,"version_major":2},"text/plain":["pos files:   0%|          | 0/12500 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9e7e42a0867a414d81f0d0aea78ae7ba","version_minor":0,"version_major":2},"text/plain":["neg files:   0%|          | 0/12500 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Created `train_dataset` with 25000 examples!\n","Created `train_dataloader` with 782 batches!\n","\n","Dealing with Validation...\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d1a18abbf5a48c7ba45e3829fcb5397","version_minor":0,"version_major":2},"text/plain":["pos files:   0%|          | 0/12500 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"abb086b26fa8410fa367ab131bb1aeb7","version_minor":0,"version_major":2},"text/plain":["neg files:   0%|          | 0/12500 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Created `valid_dataset` with 25000 examples!\n","Created `eval_dataloader` with 782 batches!\n"]}]},{"cell_type":"markdown","source":["## Training\n","\n","Let's see some examples from the dataset. Note that all reviews are truncated to just 60 tokens."],"metadata":{"id":"nXiYxK0epdFo"}},{"cell_type":"code","source":["for batch in train_dataloader:\n","    true_labels = batch['labels'].numpy().flatten().tolist()\n","    batch = {k:v.type(torch.long).to(device) for k,v in batch.items()}\n","    print(\"Dataset example (decoded):\")\n","    print(gpt2_tokenizer.decode(batch['input_ids'][0]))\n","    print(f\"This has {['negative', 'positive'][true_labels[0]]} sentiment\")\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4DbF7b0snNmg","executionInfo":{"status":"ok","timestamp":1644676034092,"user_tz":0,"elapsed":12608,"user":{"displayName":"Arthur Conmy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkoRJSBDElT-ypT7GdBmkdRp5s6AtU2rcBsGV71w=s64","userId":"06695409732812757016"}},"outputId":"0c18ba98-a695-42b3-9513-5fbaaada7762"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset example (decoded):\n","What a sad sight these TV stalwarts make, running out the clock on their careers stumbling about a little rusting hulk of a ship - boat might be more appropriate. The whole production feels cheap and shabby, and it's not helped by a \"big name\" star who is barely capable\n","This has negative sentiment\n"]}]},{"cell_type":"markdown","source":["We now load the GPT-2 model."],"metadata":{"id":"uLQ_XYnGrnQU"}},{"cell_type":"code","source":["# Get model configuration.\n","print('Loading config...')\n","model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, num_labels=n_labels)\n","\n","# Get model's tokenizer.\n","print('Loading tokenizer...')\n","tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n","# default to left padding\n","tokenizer.padding_side = \"left\"\n","# Define PAD Token = EOS Token = 50256\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","def get_model():\n","    # Get the actual model.\n","    print('Loading model...')\n","    model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, config=model_config)\n","\n","    # resize model embedding to match new tokenizer\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","    # fix model padding token id\n","    model.config.pad_token_id = model.config.eos_token_id\n","\n","    # Load model to defined device.\n","    model.to(device)\n","    print('Model loaded to `%s`'%device)\n","    return model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FMUg69Ul5x8_","executionInfo":{"status":"ok","timestamp":1644676141467,"user_tz":0,"elapsed":1503,"user":{"displayName":"Arthur Conmy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkoRJSBDElT-ypT7GdBmkdRp5s6AtU2rcBsGV71w=s64","userId":"06695409732812757016"}},"outputId":"a9ee5509-567e-44c7-950d-bf68fab09586"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading config...\n","Loading tokenizer...\n"]}]},{"cell_type":"markdown","source":["We can run the model on many sentences to check that it is randomly initialised:"],"metadata":{"id":"_UH9xuPpAo-z"}},{"cell_type":"code","source":["model = get_model()\n","validation(model, valid_dataloader, \"cuda\", 3)"],"metadata":{"id":"8y71rtEX6WAV","colab":{"base_uri":"https://localhost:8080/","height":242,"referenced_widgets":["e826cf3e6222413dbf927e7107e866ed","861743527f884356aa8efe331af3fe97","93fd28292bce400b947fa5366563e8e8","56affae1019f4594a19d0ec4f19fd531","28e8f7aa2b56407dbc40ba4b3f11c2d3","461cbd294a5c4d6692a7a4e75e43d222","17d65adb6c1c4904b26c6ee240b6c57a","961e86f1015141e8b9528bb0ebf6fce1","d93cb31dcf484f48ab1ce8336059b0a0","79242855436748468d5a38718e911dae","ea997f91690146cfa6910b57a97f9d10"]},"executionInfo":{"status":"ok","timestamp":1644676167495,"user_tz":0,"elapsed":4083,"user":{"displayName":"Arthur Conmy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkoRJSBDElT-ypT7GdBmkdRp5s6AtU2rcBsGV71w=s64","userId":"06695409732812757016"}},"outputId":"90aa1b07-2b48-478e-803d-af73935659b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Model loaded to `cuda`\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e826cf3e6222413dbf927e7107e866ed","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:207: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:208: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"output_type":"stream","name":"stdout","text":["[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","On the validation dataset, the model got 45 sentiments correct and 51 sentiments incorrect\n"]}]},{"cell_type":"markdown","source":["Here is how you can extract the outputs of the GPT-2's sentiment prediction:"],"metadata":{"id":"B1j3AtiW_Tp_"}},{"cell_type":"code","source":["model = get_model()\n","true_labels = []\n","losses = []\n","DEVICE = \"cuda\"\n","\n","for batch_idx, batch in tqdm(enumerate(train_dataloader)):\n","    loss = 0.0\n","    true_labels += batch['labels'].numpy().flatten().tolist()\n","    batch = {k:v.type(torch.long).to(DEVICE) for k,v in batch.items()}\n","    model.zero_grad()\n","    outputs = model(**batch)\n","    print(\"The output of the model has keys:\", outputs.keys())\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136,"referenced_widgets":["9317552c257e4e49beafb2a5db4ec1fe","f33c8aeb809f4a47b0a296dc97f99533","1f11c8beddf347abac9da8948fbf5196","f461c1dcc3ec436e98e5df2251456732","5289c9f9a64240bab960873c79b02ef8","b07423db200d41369ed664ee9085ba17","0e3bc1e131344005bd07dcb4ba13aa2c","f124be6d460e476bbbb323bc4ebde926","125da5a95d854eb7b97b5ba649910de5","61b06af9785547f2b5df62668cf09e92","fa87c4cbc63440d9824f8e8141d6e826"]},"id":"C7VSACbv_cty","executionInfo":{"status":"ok","timestamp":1644676175399,"user_tz":0,"elapsed":2769,"user":{"displayName":"Arthur Conmy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkoRJSBDElT-ypT7GdBmkdRp5s6AtU2rcBsGV71w=s64","userId":"06695409732812757016"}},"outputId":"45e52623-c26d-4c7d-cc34-5790bbf64127"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Model loaded to `cuda`\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9317552c257e4e49beafb2a5db4ec1fe","version_minor":0,"version_major":2},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The output of the model has keys: odict_keys(['loss', 'logits', 'past_key_values'])\n"]}]},{"cell_type":"markdown","source":["You should now have all the tools to finetune the GPT-2 for sentiment classification (which element of that dictionary will be *very* useful?).\n","\n","This task may require digging around to find how to debug things. In order to proceed we recommend using i) the above cell, as well as ii) https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop in order to setup a training loop.\n","\n","You should be able to get similar loss to the following plot in just the few iterations.\n","\n","![](https://i.imgur.com/hjtDipf.png)\n","\n","Which movie reviews does the model fail on (look at the `validate` code to see how to implement this)?\n","\n","Write a review with positive or negative sentiment - can your pretrained model classify it?"],"metadata":{"id":"LioUZ3ki-xoI"}},{"cell_type":"markdown","source":["\n","## Footnotes"],"metadata":{"id":"sxg6rf6W5gXZ"}},{"cell_type":"markdown","source":["### Resources:\n","\n","* Very visual explanation: https://jalammar.github.io/illustrated-transformer/\n","\n","* Quick summary of `transformers` library: https://neptune.ai/blog/hugging-face-pre-trained-models-find-the-best\n","\n","* Vision transformers: https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html\n","\n","* Best (?) **intuitive** explanation of attention: https://nostalgebraist.tumblr.com/post/185326092369/1-classic-fully-connected-neural-networks-these"],"metadata":{"id":"nfuH73GNxEK7"}}]}